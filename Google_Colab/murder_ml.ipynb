{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "murder_ml.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB7t7phgmCmV"
      },
      "source": [
        "# Import Dependencies \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import tensorflow as tf\n",
        "\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt \n",
        "import os"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZteeHuyM3oA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b76128e3-661f-480d-a5a7-272adfe66116"
      },
      "source": [
        "# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\n",
        "spark_version = 'spark-3.0.2'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [1 InRelease 14.2 kB/88.7\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to cloud.r-pr\r0% [1 InRelease gpgv 88.7 kB] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Connecting to cloud.r-proj\r                                                                               \rHit:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:12 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,619 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,185 kB]\n",
            "Fetched 5,056 kB in 3s (1,934 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwSA95TMOYyg"
      },
      "source": [
        "# Start a Spark session \n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.16.jar\").getOrCreate()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIEBmMWvmJ0w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b684580-07d1-4c4d-f8d5-8bb3ae47fb0f"
      },
      "source": [
        "\n",
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url =\"https://lgodleski-bucket.s3.us-east-2.amazonaws.com/murders_county_merged.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "murders_county_merged_df = spark.read.csv(SparkFiles.get(\"murders_county_merged.csv\"), sep=\",\", header=True, inferSchema=True)\n",
        "murders_county_merged_df.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-------+--------+------+------+--------+-----+-----+------+-----+-------+-------+------+---------+------------+---------------+-------+------------+--------+------------+\n",
            "|censusid| county|totalpop|   men| women|hispanic|white|black|native|asian|pacific|citizen|income|incomeerr|incomepercap|incomepercaperr|poverty|childpoverty|employed|unemployment|\n",
            "+--------+-------+--------+------+------+--------+-----+-----+------+-----+-------+-------+------+---------+------------+---------------+-------+------------+--------+------------+\n",
            "|    6001|Alameda| 1584983|776699|808284|    22.6| 33.0| 11.3|   0.3| 27.5|    0.8|1025865| 75619|      613|       37285|            279|   12.5|        15.2|  778132|         8.3|\n",
            "|    6001|Alameda| 1584983|776699|808284|    22.6| 33.0| 11.3|   0.3| 27.5|    0.8|1025865| 75619|      613|       37285|            279|   12.5|        15.2|  778132|         8.3|\n",
            "|    6001|Alameda| 1584983|776699|808284|    22.6| 33.0| 11.3|   0.3| 27.5|    0.8|1025865| 75619|      613|       37285|            279|   12.5|        15.2|  778132|         8.3|\n",
            "|    6001|Alameda| 1584983|776699|808284|    22.6| 33.0| 11.3|   0.3| 27.5|    0.8|1025865| 75619|      613|       37285|            279|   12.5|        15.2|  778132|         8.3|\n",
            "|    6001|Alameda| 1584983|776699|808284|    22.6| 33.0| 11.3|   0.3| 27.5|    0.8|1025865| 75619|      613|       37285|            279|   12.5|        15.2|  778132|         8.3|\n",
            "|    6001|Alameda| 1584983|776699|808284|    22.6| 33.0| 11.3|   0.3| 27.5|    0.8|1025865| 75619|      613|       37285|            279|   12.5|        15.2|  778132|         8.3|\n",
            "|    6001|Alameda| 1584983|776699|808284|    22.6| 33.0| 11.3|   0.3| 27.5|    0.8|1025865| 75619|      613|       37285|            279|   12.5|        15.2|  778132|         8.3|\n",
            "|    6001|Alameda| 1584983|776699|808284|    22.6| 33.0| 11.3|   0.3| 27.5|    0.8|1025865| 75619|      613|       37285|            279|   12.5|        15.2|  778132|         8.3|\n",
            "|    6001|Alameda| 1584983|776699|808284|    22.6| 33.0| 11.3|   0.3| 27.5|    0.8|1025865| 75619|      613|       37285|            279|   12.5|        15.2|  778132|         8.3|\n",
            "|    6001|Alameda| 1584983|776699|808284|    22.6| 33.0| 11.3|   0.3| 27.5|    0.8|1025865| 75619|      613|       37285|            279|   12.5|        15.2|  778132|         8.3|\n",
            "|    6001|Alameda| 1584983|776699|808284|    22.6| 33.0| 11.3|   0.3| 27.5|    0.8|1025865| 75619|      613|       37285|            279|   12.5|        15.2|  778132|         8.3|\n",
            "|    6001|Alameda| 1584983|776699|808284|    22.6| 33.0| 11.3|   0.3| 27.5|    0.8|1025865| 75619|      613|       37285|            279|   12.5|        15.2|  778132|         8.3|\n",
            "|    6001|Alameda| 1584983|776699|808284|    22.6| 33.0| 11.3|   0.3| 27.5|    0.8|1025865| 75619|      613|       37285|            279|   12.5|        15.2|  778132|         8.3|\n",
            "|    6001|Alameda| 1584983|776699|808284|    22.6| 33.0| 11.3|   0.3| 27.5|    0.8|1025865| 75619|      613|       37285|            279|   12.5|        15.2|  778132|         8.3|\n",
            "|    6001|Alameda| 1584983|776699|808284|    22.6| 33.0| 11.3|   0.3| 27.5|    0.8|1025865| 75619|      613|       37285|            279|   12.5|        15.2|  778132|         8.3|\n",
            "|    6001|Alameda| 1584983|776699|808284|    22.6| 33.0| 11.3|   0.3| 27.5|    0.8|1025865| 75619|      613|       37285|            279|   12.5|        15.2|  778132|         8.3|\n",
            "|    6001|Alameda| 1584983|776699|808284|    22.6| 33.0| 11.3|   0.3| 27.5|    0.8|1025865| 75619|      613|       37285|            279|   12.5|        15.2|  778132|         8.3|\n",
            "|    6001|Alameda| 1584983|776699|808284|    22.6| 33.0| 11.3|   0.3| 27.5|    0.8|1025865| 75619|      613|       37285|            279|   12.5|        15.2|  778132|         8.3|\n",
            "|    6001|Alameda| 1584983|776699|808284|    22.6| 33.0| 11.3|   0.3| 27.5|    0.8|1025865| 75619|      613|       37285|            279|   12.5|        15.2|  778132|         8.3|\n",
            "|    6001|Alameda| 1584983|776699|808284|    22.6| 33.0| 11.3|   0.3| 27.5|    0.8|1025865| 75619|      613|       37285|            279|   12.5|        15.2|  778132|         8.3|\n",
            "+--------+-------+--------+------+------+--------+-----+-----+------+-----+-------+-------+------+---------+------------+---------------+-------+------------+--------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZCMO97BaKiH",
        "outputId": "1b58734a-9477-4ca0-c1ed-6982f9f066ed"
      },
      "source": [
        "# Convert PySpark DataFrame to Pandas DataFrame\n",
        "murders_df = murders_county_merged_df.toPandas()\n",
        "murders_df.dtypes"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "censusid             int32\n",
              "county              object\n",
              "totalpop             int32\n",
              "men                  int32\n",
              "women                int32\n",
              "hispanic           float64\n",
              "white              float64\n",
              "black              float64\n",
              "native             float64\n",
              "asian              float64\n",
              "pacific            float64\n",
              "citizen              int32\n",
              "income               int32\n",
              "incomeerr            int32\n",
              "incomepercap         int32\n",
              "incomepercaperr      int32\n",
              "poverty            float64\n",
              "childpoverty       float64\n",
              "employed             int32\n",
              "unemployment       float64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE-PEVpZwQwf"
      },
      "source": [
        "# Generate our categorical variable list \n",
        "murder_cat = murders_df.dtypes[murders_df.dtypes == \"object\"].index.tolist()\n",
        "murder_cat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUybpwn_oZ9W"
      },
      "source": [
        "# Create a OneHotEncoder instance\n",
        "enc = OneHotEncoder(sparse=False)\n",
        "\n",
        "# Fit and transform the OneHotEncoder using the categorical variable list\n",
        "encode_df = pd.DataFrame(enc.fit_transform(murders_df[murder_cat]))\n",
        "\n",
        "# Add the encoded variable names to the DataFrame\n",
        "encode_df.columns = enc.get_feature_names(murder_cat)\n",
        "encode_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTyBGj-opjCz"
      },
      "source": [
        "# Merge one-hot encoded features and drop the originals\n",
        "murders_df = murders_df.merge(encode_df,left_index=True, right_index=True)\n",
        "murders_df = murders_df.drop(murder_cat,1)\n",
        "murders_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_w_a4qZp_EC"
      },
      "source": [
        "# Remove 'CrimeSolved' target from features data\n",
        "y = murders_df.CrimeSolved\n",
        "X = loans_df.drop(columns=[\"CrimeSolved\"])\n",
        "\n",
        "# Split training/test datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=89, stratify=y)\n",
        "\n",
        "# Create a StandardScaler instance\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the StandardScaler\n",
        "X_scaler = scaler.fit(X_train)\n",
        "\n",
        "# Scale the data\n",
        "X_train_scaled = X_scaler.transform(X_train)\n",
        "X_test_scaled = X_scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjMDR7sRwTMe"
      },
      "source": [
        "## Random Forest Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr7n4SrXqYBF"
      },
      "source": [
        "# Create a random forest classifier.\n",
        "rf_model = RandomForestClassifier(n_estimators=500, random_state=89)\n",
        "\n",
        "# Fitting the model\n",
        "rf_model = rf_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = rf_model.predict(X_test_scaled)\n",
        "print(f'Random forest training score: {rf_model.score(X_train_scaled, y_train)}')\n",
        "print(f'Random forest testing score: {rf_model.score(X_test_scaled, y_test)}')\n",
        "print(f\" Random forest predictive accuracy: {accuracy_score(y_test,y_pred):.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hREvb86Xtbwn"
      },
      "source": [
        "# Rank features by importance and plot \n",
        "features = rf_model.feature_importances_\n",
        "print(features)\n",
        "plt.bar(x = range(len(features)), height=features)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adz58eH8BmpP"
      },
      "source": [
        "##Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvVX1XaLBtDN"
      },
      "source": [
        "# Create a logistic regression model.\n",
        "logr_model = LogisticRegression()\n",
        "\n",
        "# Fitting the model. \n",
        "logr_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate the model. \n",
        "print(f'Logistic regression training Score: {logr_model.score(X_train_scaled, y_train)}')\n",
        "print(f'Logistic regression testing Score: {logr_model.score(X_test_scaled, y_test)}')\n",
        "print(f\" Logistic regression predictive accuracy: {accuracy_score(y_test,y_pred):.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMp6R2HTEfu8"
      },
      "source": [
        "## Deep Neural Net Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3djHIyvFHO8"
      },
      "source": [
        "# Define the model - deep neural net\n",
        "number_input_features = len(X_train_scaled[0])\n",
        "# Change number of nodes to reflect this dataset: layer1 + layer 2 should equal # of columns after merge/drop OneHotEncoding \n",
        "hidden_nodes_layer1 =  24\n",
        "hidden_nodes_layer2 = 12\n",
        "\n",
        "nn = tf.keras.models.Sequential()\n",
        "\n",
        "# First hidden layer\n",
        "nn.add(\n",
        "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
        ")\n",
        "\n",
        "# Second hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
        "\n",
        "\n",
        "# Output layer\n",
        "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "# Compile the Sequential model together and customize metrics\n",
        "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "fit_model = nn.fit(X_train_scaled, y_train, epochs=50)\n",
        "\n",
        "# Evaluate the model using the test data\n",
        "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
        "print(f'deep neural net training Score: {nn_model.score(X_train_scaled, y_train)}')\n",
        "print(f'deep neural net testing Score: {nn.score(X_test_scaled, y_test)}')\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}